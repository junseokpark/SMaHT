{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f6c2f1e-029b-432b-a913-511fe50044f6",
   "metadata": {},
   "source": [
    "# Overall Explanation of Generating Simulated Sequence File "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9e1d9a-b3b1-45f5-84a0-04a9b4d97ee4",
   "metadata": {},
   "source": [
    "explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jp394/miniforge3/envs/SMaTH/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "692f924a-f79c-4d64-990f-968027a53de8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/jp394/lee/projects/SMaHT/analysis/3.generateSimulatedData.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bo2-jump/home/jp394/lee/projects/SMaHT/analysis/3.generateSimulatedData.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgraphviz\u001b[39;00m \u001b[39mimport\u001b[39;00m Digraph\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9f8d3a13-e6ad-47ed-9e4e-1843b509f641",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraphviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Digraph\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create a Digraph object\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "from IPython.display import Image\n",
    "\n",
    "# Create a Digraph object\n",
    "dot = Digraph(comment='Simple Flow Chart')\n",
    "\n",
    "# Add nodes and edges to the graph\n",
    "dot.node('A', 'Start')\n",
    "dot.node('B', 'Step 1')\n",
    "dot.node('C', 'Step 2')\n",
    "dot.node('D', 'End')\n",
    "\n",
    "dot.edges(['AB', 'BC', 'CD'])\n",
    "\n",
    "# Render the graph to a PNG file\n",
    "dot.render('flow_chart', format='png', cleanup=True)\n",
    "\n",
    "# Display the PNG file in the notebook\n",
    "Image(filename='flow_chart.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8018a6ba-e78e-493b-9df5-9b8b13428272",
   "metadata": {},
   "source": [
    "# User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4835c03-9986-4393-811e-2851527fa365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vcf\n",
    "import subprocess\n",
    "import pysam\n",
    "import pyranges \n",
    "\n",
    "# Load VCF Files\n",
    "\n",
    "def vcf_to_bed(vcf_path, bed_path):\n",
    "    # Open VCF file\n",
    "    vcf_reader = vcf.Reader(open(vcf_path, 'r'))\n",
    "\n",
    "    # Open BED file for writing\n",
    "    with open(bed_path, 'w') as bed_file:\n",
    "        # Iterate through VCF records and write to BED file\n",
    "        for record in vcf_reader:\n",
    "            chrom = record.CHROM\n",
    "            start = record.POS - 1  # Convert to 0-based coordinates for BED format\n",
    "            end = record.POS\n",
    "            info = record.INFO\n",
    "\n",
    "            bed_file.write(f\"{chrom}\\t{start}\\t{end}\\t{info}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9f32db2-ff6a-42ba-b266-48fcfa1647d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original source code is from https://www.biostars.org/p/13516/\n",
    "\n",
    "from collections import namedtuple, defaultdict\n",
    "from pybedtools import BedTool\n",
    "import argparse\n",
    "\n",
    "Point    = namedtuple('Point', ['id', 'pos', 'type'])\n",
    "Interval = namedtuple('Interval', ['chrom', 'start', 'end'])\n",
    "\n",
    "\n",
    "def report_interval(chrom, start, end, num_files, files_with_interval):\n",
    "    print(\"\\t\".join([chrom, str(start), str(end), str(len(files_with_interval.keys()))]), end=\"\")\n",
    "    for i in range(0, num_files):\n",
    "        if i in files_with_interval:\n",
    "            print(\"\\t1\", end=\"\")\n",
    "        else:\n",
    "            print(\"\\t0\", end=\"\")\n",
    "\n",
    "\n",
    "def merge(file):\n",
    "    \"\"\"\n",
    "    Merge features in a BED/GFF/VCF into non-overlapping intervals\n",
    "    \"\"\"\n",
    "    start = -1\n",
    "    end   = -1\n",
    "    chrom = None\n",
    "    for feature in BedTool(file):\n",
    "        if feature.start - end > 0 or end < 0 or feature.chrom != chrom:\n",
    "            if start >= 0:\n",
    "                yield Interval(chrom, start, end)\n",
    "            start = feature.start\n",
    "            end   = feature.end\n",
    "            chrom = feature.chrom\n",
    "        elif feature.end > end:\n",
    "            end = feature.end\n",
    "    yield Interval(chrom, start, end)\n",
    "\n",
    "\n",
    "def load_and_sort_points(files):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    file_id = 0\n",
    "    chrom_points = defaultdict(list)\n",
    "    # for each input file, first merge the features into non-overlapping\n",
    "    # intervals using merge().  Each non-overlapping feature is then \n",
    "    # broken up into discrete \"Points\": one for the start and one for the end.\n",
    "    for file in files:\n",
    "        # merge the file and split features into points\n",
    "        for feature in merge(file):\n",
    "            s = Point(file_id, feature.start, \"start\")\n",
    "            e = Point(file_id, feature.end,   \"end\")\n",
    "            chrom_points[feature.chrom].append(s)\n",
    "            chrom_points[feature.chrom].append(e)\n",
    "        file_id += 1\n",
    "    \n",
    "    # sort the points in for each chrom \n",
    "    for chrom in chrom_points:\n",
    "        chrom_points[chrom].sort(key=lambda i: i.pos)\n",
    "    return chrom_points\n",
    "\n",
    "\n",
    "def load_genome(genome):\n",
    "    chrom_sizes = {}\n",
    "    for line in open(genome, 'r'):\n",
    "        fields = line.strip().split(\"\\t\")\n",
    "        if len(fields) > 1:\n",
    "            chrom_sizes[fields[0]] = fields[1]\n",
    "\n",
    "    return chrom_sizes\n",
    "\n",
    "\n",
    "def nway(files, genome):\n",
    "    \"\"\"\n",
    "    Assumptions: input files must contain non-overlapping intervals\n",
    "    \n",
    "    1. Example using already-merged files:\n",
    "    $ cat a.merged \n",
    "    chr1\t6\t20\n",
    "    chr1\t22\t30\n",
    "    \n",
    "    $ cat b.merged \n",
    "    chr1\t12\t32\n",
    "    \n",
    "    $ cat c.merged \n",
    "    chr1\t8\t15\n",
    "    chr1\t32\t34\n",
    "    \n",
    "    \n",
    "    $ ./nway-cluster.py a.merged b.merged c.merged\n",
    "    #chr\tst\ted\tnum\ta\tb\tc \n",
    "    chr1\t0\t6 \t0 \t0 \t0 \t0\n",
    "    chr1\t6\t8 \t1 \t1 \t0 \t0\n",
    "    chr1\t8\t12 \t2 \t1 \t0 \t1\n",
    "    chr1\t12\t15 \t3 \t1 \t1 \t1\n",
    "    chr1\t15\t20 \t2 \t1 \t1 \t0\n",
    "    chr1\t20\t22 \t1 \t0 \t1 \t0\n",
    "    chr1\t22\t30 \t2 \t1 \t1 \t0\n",
    "    chr1\t30\t32 \t1 \t0 \t1 \t0\n",
    "    chr1\t32\t34 \t1 \t0 \t0 \t1\n",
    "    \n",
    "    \n",
    "    2. Example using un-merged, yet sorted files:\n",
    "    $ cat a.bed \n",
    "    chr1\t6\t12\n",
    "    chr1\t10\t20\n",
    "    chr1\t22\t27\n",
    "    chr1\t24\t30\n",
    "    \n",
    "    $ cat b.bed \n",
    "    chr1\t12\t32\n",
    "    chr1\t14\t30\n",
    "    \n",
    "    $ cat c.bed \n",
    "    chr1\t8\t15\n",
    "    chr1\t10\t14\n",
    "    chr1\t32\t34\n",
    "    \n",
    "    $ ./nway-cluster.py a.bed b.bed c.bed\n",
    "    #chr\tst\ted\tnum\ta\tb\tc \n",
    "    chr1\t0\t6 \t0 \t0 \t0 \t0\n",
    "    chr1\t6\t8 \t1 \t1 \t0 \t0\n",
    "    chr1\t8\t12 \t2 \t1 \t0 \t1\n",
    "    chr1\t12\t15 \t3 \t1 \t1 \t1\n",
    "    chr1\t15\t20 \t2 \t1 \t1 \t0\n",
    "    chr1\t20\t22 \t1 \t0 \t1 \t0\n",
    "    chr1\t22\t30 \t2 \t1 \t1 \t0\n",
    "    chr1\t30\t32 \t1 \t0 \t1 \t0\n",
    "    chr1\t32\t34 \t1 \t0 \t0 \t1\n",
    "    \n",
    "    \n",
    "    3. Thanks to pybedtools, it works with BAM files as well.\n",
    "       But I hope you have a machine with lots of RAM.\n",
    "    ./nway-cluster.py 1.bam 2.bam 3.bam\n",
    "    \n",
    "    \"\"\"\n",
    "    num_files = len(files)\n",
    "    \n",
    "    # 1. load each point from each interval in each file into\n",
    "    #    a hash keyed by chrom.  \n",
    "    # 2. sort the points in asecnding order for each chrom\n",
    "    chrom_points = load_and_sort_points(files)\n",
    "    if genome is not None:\n",
    "        chrom_sizes  = load_genome(genome)\n",
    "    \n",
    "    # 3. Rip through the points and find shared intervals\n",
    "    for chrom in chrom_points:\n",
    "        files_with_interval = {}\n",
    "        prev_point = 0\n",
    "        for point in chrom_points[chrom]:\n",
    "            # report the current interval if we've moved at all along the chrom.\n",
    "            if point.pos > prev_point:\n",
    "                report_interval(chrom, prev_point, point.pos, num_files, files_with_interval)\n",
    "            # if we're at a start, we add the current file to the active list of files. \n",
    "            # otherwise, an end point means we can drop the current file.\n",
    "            if point.type == \"start\":\n",
    "                files_with_interval[point.id] = 1\n",
    "            else:\n",
    "                del files_with_interval[point.id]\n",
    "            prev_point = point.pos\n",
    "\n",
    "        # if requested, handle the interval from the last observed point to the end of the chrom\n",
    "        if genome is not None and point.pos < chrom_sizes[chrom]:\n",
    "            report_interval(chrom, point.pos, chrom_sizes[chrom], num_files, files_with_interval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc8f873-e175-4013-83e0-fad0907012f5",
   "metadata": {},
   "source": [
    "# 1. Bed File Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14730e29-9c18-4fe0-b8c0-51a4f13aac65",
   "metadata": {},
   "source": [
    "## 1.1. Generate Bed File from VCF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20c8d46d-60a0-4009-bf77-4d1531753583",
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf_to_bed(\"results/xTea/shortread/mosaic/GIAB/Illumina300x/HG002/Alu/HG002.GRCh38.300x_ALU.vcf\", \"results/xTea/shortread/mosaic/GIAB/Illumina300x/HG002/Alu/HG002.GRCh38.300x_ALU.bed\")\n",
    "vcf_to_bed(\"results/xTea/shortread/mosaic/GIAB/Illumina300x/HG003/Alu/HG003.GRCh38.300x_ALU.vcf\", \"results/xTea/shortread/mosaic/GIAB/Illumina300x/HG003/Alu/HG003.GRCh38.300x_ALU.bed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0908a98d-ccb1-47a8-837b-e837988eb302",
   "metadata": {},
   "source": [
    "## 1.2. Load Bed Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf1103a-d631-4d50-851d-bfd5a42c583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyranges.read_bed(\"results/xTea/shortread/mosaic/GIAB/Illumina300x/HG003/Alu/HG003.GRCh38.300x_ALU.bed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44199bd9-726a-46ea-a5a8-823501c967c9",
   "metadata": {},
   "source": [
    "## 1.3. Calculate Intersect between two bed files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57c88e9-5a47-4ea0-8c00-87013c4d7267",
   "metadata": {},
   "source": [
    "## 1.4. Calculate AF from the bam file based on bed file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2187776a-b5b5-49a6-be5a-d53838d6d4f4",
   "metadata": {},
   "source": [
    "# Remove Spikes from BAM File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa891b36-462a-48cf-8d4e-d961634c3e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_spikes(input_bam, bed_file, output_bam):\n",
    "    try:\n",
    "        # Open input BAM file\n",
    "        with pysam.AlignmentFile(input_bam, \"rb\") as input_bam_file:\n",
    "            # Open BED file for regions to keep\n",
    "            with open(bed_file, \"r\") as bed:\n",
    "                # Create a list of regions from the BED file\n",
    "                regions = [line.strip().split('\\t') for line in bed]\n",
    "\n",
    "                # Fetch reads overlapping the specified regions\n",
    "                filtered_reads = input_bam_file.fetch(regions=regions)\n",
    "\n",
    "                # Open output BAM file for writing\n",
    "                with pysam.AlignmentFile(output_bam, \"wb\", header=input_bam_file.header) as output_bam_file:\n",
    "                    # Write filtered reads to output BAM file\n",
    "                    for read in filtered_reads:\n",
    "                        output_bam_file.write(read)\n",
    "\n",
    "        print(f\"Spikes removed successfully. Output saved to {output_bam}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "def extract_regions(input_bam, bed_file, output_bam):\n",
    "    # Step 2: Extract regions from the first BAM file\n",
    "    with pysam.AlignmentFile(input_bam, \"rb\") as bam_file:\n",
    "        regions = pysam.TabixFile(bed_file)\n",
    "        extracted_reads = [read for read in bam_file.fetch() if regions.fetch(read.reference_name, read.reference_start, read.reference_end)]\n",
    "\n",
    "    # Write the extracted reads to a new BAM file\n",
    "    with pysam.AlignmentFile(output_bam, \"wb\", header=bam_file.header) as output_bam_file:\n",
    "        for read in extracted_reads:\n",
    "            output_bam_file.write(read)\n",
    "\n",
    "\n",
    "def simulate_insertions(reference_genome, extracted_bam, output_prefix):\n",
    "    try:\n",
    "        # Open reference genome\n",
    "        with pysam.FastaFile(reference_genome) as ref_fasta:\n",
    "            # Open extracted BAM file\n",
    "            with pysam.AlignmentFile(extracted_bam, \"rb\") as input_bam_file:\n",
    "                # Open output BAM file for writing simulated reads\n",
    "                with pysam.AlignmentFile(output_prefix + \".bam\", \"wb\", header=input_bam_file.header) as output_bam_file:\n",
    "                    # Iterate over reads in the input BAM file\n",
    "                    for read in input_bam_file:\n",
    "                        # Simulate insertions (modify read sequence, qualities, etc. as needed)\n",
    "                        simulated_read = simulate_insertion(read, ref_fasta)\n",
    "\n",
    "                        # Write the simulated read to the output BAM file\n",
    "                        output_bam_file.write(simulated_read)\n",
    "\n",
    "        print(f\"Insertions simulated successfully. Output saved to {output_prefix}.bam\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "def simulate_insertion(read, ref_fasta):\n",
    "    # Modify the read to simulate an insertion (customize based on your needs)\n",
    "    # For example, you can extend the read sequence, qualities, etc.\n",
    "    simulated_read = read.copy()\n",
    "    simulated_read.query_sequence += \"INSERTED_SEQUENCE\"\n",
    "    simulated_read.query_qualities += [30] * len(\"INSERTED_SEQUENCE\")\n",
    "\n",
    "    # Adjust alignment information if necessary\n",
    "    simulated_read.reference_end += len(\"INSERTED_SEQUENCE\")\n",
    "\n",
    "    return simulated_read\n",
    "\n",
    "\n",
    "def merge_bams(original_bam, simulated_bam, merged_bam):\n",
    "    # Step 4: Merge BAM files\n",
    "    with pysam.AlignmentFile(original_bam, \"rb\") as original_file, \\\n",
    "         pysam.AlignmentFile(simulated_bam + '.sam', \"rb\") as simulated_file, \\\n",
    "         pysam.AlignmentFile(merged_bam, \"wb\", header=original_file.header) as output_bam:\n",
    "        for read in original_file:\n",
    "            output_bam.write(read)\n",
    "        for read in simulated_file:\n",
    "            output_bam.write(read)\n",
    "\n",
    "    # Step 5: Sort and index merged BAM file\n",
    "    pysam.sort('-o', merged_bam.replace('.bam', '_sorted.bam'), merged_bam)\n",
    "    pysam.index(merged_bam.replace('.bam', '_sorted.bam'))\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Specify file paths\n",
    "#     input_bam = \"/path/to/your/original.bam\"\n",
    "#     bed_file = \"/path/to/your/simulated_insertions.bed\"\n",
    "#     output_bam = \"/path/to/your/extracted_reads.bam\"\n",
    "#     reference_genome = \"/path/to/your/reference_genome.fasta\"\n",
    "#     output_prefix = \"/path/to/your/simulated_insertions\"\n",
    "#     merged_bam = \"/path/to/your/final_merged.bam\"\n",
    "\n",
    "#     # Extract regions from the original BAM file\n",
    "#     extract_regions(input_bam, bed_file, output_bam)\n",
    "\n",
    "#     # Simulate insertions\n",
    "#     simulate_insertions(reference_genome, output_bam, output_prefix)\n",
    "\n",
    "#     # Merge the original and simulated BAM files\n",
    "#     merge_bams(input_bam, output_prefix + '.bam', merged_bam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca923d1c-d312-4400-b805-1975dce92dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SMaTH",
   "language": "python",
   "name": "smath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
